
\section{Related Work}
\label{sec:related-work}

%We categorize the related work into subsections as follows:

\subsection{NMT Architectures}
\label{sec:rel-nmt-arch}
Several variations of NMT models have been proposed and refined: \citet{sutskever2014seq2seq} and \citet{cho2014learning} introduce the RNN-based encoder-decoder model.
\citet{bahdanau2014nmtattn} introduce the attention mechanism and \citet{luong2015effectiveAttn} propose several variations that became essential components of many future models.
RNN modules, either LSTM \cite{hochreiter1997LSTM} or GRU \cite{cho-etal-2014-properties}, have been popular choices for composing NMT encoders and decoders.
The encoder uses bidirectional information, but the decoder is unidirectional, typically left-to-right, to facilitate autoregressive generation.
\citet{gehring2017CNNMT} use a CNN architecture that outperforms RNN models.
\citet{vaswani2017attention} propose the \textbf{Transformer}, whose main components are feed-forward and attention networks.
Our experiments and analysis are based on Transformer NMT architecture, however our proposed abstraction of NMT as a combination of Classifier and an Autoregressor is applicable to other autoregressive NMT architectures that have the objective of maximizing $P(y_t | y_{<t}, x_{1:m})$.
There are only a few models that perform non-autoregressive NMT \cite{libovicky-helcl-2018-end,Gu-etal-17-NonAR-NMT}.
These are focused on improving the speed of inference; generation quality is currently sub-par compared to autoregressive models.
These non-autoregressive models can also be viewed as token classifiers with a different kind of feature extractor, whose strengths and limitations are yet to be theoretically understood.
Analyzing the non-autoregressive component, especially its performance with longer sequences, is beyond the scope of this work.

\subsection{BPE Subwords}
\citet{sennrich-etal-2016-bpe} introduce BPE as a simplified way to solve out-of-vocabulary (OOV) words without having to use a back-off dictionary for OOV words.
They note that BPE improves the translation of not only the OOV words, but also some rare in-vocabulary words.
%In their work, the vocabulary size was arbitrary, and large as $60k$ and $100k$.
The analysis by \citet{morishita-etal-2018-improving} is different than ours in that they view various vocabulary sizes as hierarchical features that are used in addition to a fixed vocabulary.
\citet{DBLP:journals/corr/abs-1810-08641} offer an efficient way to search BPE vocabulary size for NMT.
\citet{kudo-2018-subword} use BPE as a regularization technique by introducing sampling based randomness to the BPE segmentation.
To the best of our knowledge, no previous work exists that analyzes BPE's effect on class imbalance.% or answered \textit{`why are certain BPE vocabularies better than others?'}. % probably because the weaknesses of classification module in NMT was overlooked.

\subsection{Class Imbalance}
\label{sec:rel-class-imb}
The class imbalance problem has been extensively studied in classical machine learning \cite{japkowicz2002ClassImbalance}.
In the medical domain, \newcite{Maciej2008MedicalImbalance} find that classifier performance deteriorates with even modest imbalance in the training data.
%\newcite{Li:2004:SVM-IE-weight} addressed unbalanced labels in the information extraction task using an uneven margin SVM classifier.
Untreated class imbalance has been known to deteriorate the performance of image segmentation.  \newcite{Sudre2017GeneralizedDice} investigate the sensitivity of various loss functions.
%\info{ha! love it! `if you suffer from untreated class imbalance talk to your doctor about bpecil, now available in extra-strength.'}
\newcite{Johnson2019SurveyImbalance} survey imbalance learning and report that the effort is mostly targeted to computer vision tasks.
\newcite{buda-etal-2018-imbalance-cnn} provide a definition and quantification method for two types of class imbalance: \textit{step imbalance} and \textit{linear imbalance}.
%Step imbalance is suitable when there is a clear distinction between majority and minority classes. Linear imbalance is best suited when class imbalance has a linear relation.
Since the imbalance in Zipfian distribution of classes is neither single-stepped nor linear, we use a divergence based measure to quantify the imbalance.


\subsection{MT Metrics}
 Many metrics have been proposed for MT evaluation, which we broadly categorize into \textit{model-free} or \textit{model-based}. Model-free metrics compute scores based on translations but have no significant parameters or hyperparameters that must be tuned \textit{a priori}; these include  \bleu\ \cite{papineni-etal-2002-bleu}, NIST \cite{doddington2002-nist}, TER \cite{snover2006TER}, and \chrf1 \cite{popovic-2015-chrf}.  Model-based metrics have a significant number of parameters and, sometimes, external resources that must be set prior to use. These include METEOR \cite{banerjee-lavie-2005-meteor},  BLEURT \cite{sellam-etal-2020-bleurt}, YiSi \cite{lo-2019-yisi}, ESIM \cite{mathur-etal-2019-ESIM}, and BEER \cite{stanojevic-simaan-2014-beer}. Model-based metrics require significant effort and resources when adapting to a new language or domain, while model-free metrics require only a test set with references. 
\citet{mathur-etal-2020-tangled} have recently evaluated the utility of popular metrics and recommend the use of either \chrf1 or a model-based metric instead of \bleu. 
%We compare our \maf1 and \mif1 metrics with \bleu, \chrf1, and BLEURT \cite{sellam-etal-2020-bleurt}.
%Note from Figure~\ref{fig:bleu-damage} that \bleu\ and \chrf1 are implicitly micro-averaged measures similar to \mif1.

%While the \textit{model-based} methods are an interesting research direction, we are concerned about the following two key issues.
%Firstly, regarding the \textit{undesired biases}: model-based methods are based on learned representations~\cite{kaiwei-NIPS2016-emb-bias} and language modeling~\cite{sheng-etal-2019-nlg-bias} which are known to possess undesired data-based biases~\cite{mehrabi2019survey-bias}.
%A bias of our primary concern is a marginalization of rare and minority concepts that affect the generation of rare words from Long Tail \cite{gowda2020finding}.
%Secondly, regarding \textit{uninterpretability}: model-based methods offer only the scores without any reliably established way to reason about those scores. 
%if we are asked with ``why an output scored a certain score", the explanation one can provide is -- ``the evaluator model assigned that score and we do not know why". 
%Since many MT models are themselves uninterpretable and known to possess biases~\cite{prates2019-mt-bias}, using uninterpretable and possibly biased evaluators in concurrence makes the discovery and addressing of flaws in MT models even harder.
%Such a faith on uninterpretable entities in evaluation is against the spirit of science.
%The methods we have proposed in this work are simple, interpretable, based on well established classifier evaluation methods, and offer performance breakdown to the level of individual types that assist in a detailed analysis. 

%An example analysis between supervised and unsupervised NMT is given in Section~\ref{sec:unmt}.
    

\subsection{Rare Words are Important}
\label{sec:rare-words}
That natural language word types roughly follow a Zipfian distribution is a well known phenomenon \cite{zipf1949human,powers-1998-zipf-apps}.
The frequent types are mainly so-called ``stop words,'' function words, and other low-information types, while most content words are infrequent types.
%whereas the right side contains rare content words.
Even though frequent types occur several orders of magnitude more frequently than others, they carry relatively less information~\cite{shannon1948mathematical}.
To counter this natural frequency-based imbalance, statistics such as inverted document frequency (IDF) are commonly used to weigh the \textit{input} words in applications such as information retrieval~\cite{Jones72specificity}.
%IDF, being inversely proportional to frequencies, emphasizes the infrequent types, as they are more `useful' than the frequent types.
In NLG tasks such as MT, where words are the \textit{output} of a classifier, there has been scant effort to address the imbalance.
\citet{doddington2002-nist} is the only work we know of in which the `information' of an n-gram is used as its weight, such that rare n-grams attain relatively more importance than in BLEU. 
We abandon this direction for two reasons:
Firstly, as noted in that work, \textit{large amounts of data are required to estimate n-gram statistics}.
Secondly, unequal weighing is a bias that is best suited to datasets where the weights are derived from, and such biases often do not generalize to other datasets.
Therefore, unlike \citet{doddington2002-nist}, we assign equal weights to all n-gram classes, and in this work we limit our scope to unigrams only.

While \bleu{} is a precision oriented measure, METEOR \cite{banerjee-lavie-2005-meteor} and CHRF \cite{popovic-2015-chrf} include both precision and recall similar to our methods.
However, neither of these measures try to address the natural imbalance of class distribution. 
BEER \cite{stanojevic-simaan-2014-beer} and METEOR \cite{denkowski-lavie-2011-meteor1.3} make an explicit distinction between function and content words; such a distinction inherently captures the frequency differences since the function words are often frequent and content words are often infrequent types. However, doing so requires the construction of potentially expensive linguistic resources. This work does not make any explicit  distinction and uses naturally occurring type counts to effect a similar result.

\subsection{F-measure as an Evaluation Metric}
F-measure \cite{Rijsbergen-1979-F-meas, chinchor-1992-F-meas} is extensively used as an evaluation metric in classification tasks such as part-of-speech tagging, information extraction, named entity recognition, and sentiment analysis \cite{derczynski-2016-f-score}.
Viewing MT as a multi-class classifier and evaluating MT solely as a multi-class classifier as proposed in this work is not an established practice.
However, $F_1$ measure is sometimes used for various analyses when \bleu{} and others are inadequate: The compare-mt tool \citep{neubig-etal-2019-compareMT} supports comparison of MT models based on $F_1$ measure of individual types.
\citet{sennrich-etal-2016-bpe} use corpus-level \textit{unigram $F_1$} in addition to \bleu\ and \chrf{}, however, corpus-level $F_1$ is computed as \mif1. %In this work, \maf1 is our primary focus, and we use \mif1 for  only.
%\citet{gowda2020finding} use $F_1$ of individual types to uncover frequency-based bias in MT models.
To the best of our knowledge, there is no previous work that clearly formulates the differences between micro- and macro- averages, and justifies the use of \maf1 for MT evaluation. 
