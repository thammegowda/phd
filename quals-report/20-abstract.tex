Neural machine translation (NMT) models are commonly viewed as encoder-decoder networks; such a view is useful for implementation, however, inadequate for theoretical analysis. % such as why certain vocabulary sizes are better than others.
In this work, we cast NMT as a \textit{classification} task in an \textit{autoregressive} setting and analyze the limitations of both classification and autoregression components.
Balanced class distribution during training is known to improve the classifier performance, whereas imbalanced class distribution is known to induce unwelcome frequency-based biases on classes.
Since the Zipfian nature of languages causes imbalanced classes, we explore its effect and find that frequency-based class biases exist in NMT models. 
Using our proposed abstraction of NMT as classifier and autoregressor components, we analyze the effect of various vocabulary sizes on the end performance of NMT on multiple languages with many data sizes, and reveal an explanation for \textit{why} certain vocabulary sizes are better than others.
Furthermore, recently, model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement to traditional corpus-level evaluation metrics.
However, these metrics are costly, their decisions are inherently non-transparent and they appear to reflect unwelcome biases. 
We evaluate NMT using the well known classifier evaluation metrics that are easy to compute, transparent, and hyperparameter-free, and find that \maf1, a metric commonly used for evaluating classifiers on imbalanced test sets, is competitive on direct human assessment and outperforms others as a performance indicator of a downstream task. 
\footnote{Tools, configurations, system outputs, and analyses are at \href{https://github.com/thammegowda/005-nmt-imbalance}{https://github.com/thammegowda/005-nmt-imbalance} and \href{https://github.com/thammegowda/007-mt-eval-macro}{https://github.com/thammegowda/007-mt-eval-macro}} 
