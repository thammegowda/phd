

\section{Agreement with WMT Human judgments}
\label{sec:apphuman}

Tables \ref{tab:wmt19-kendall}, \ref{tab:wmt18-kendall}, and \ref{tab:wmt17-kendall} provide $\tau$ between MT metrics and human judgments on WMT Metrics task 2017--2019. 
$\star$\bleu\ is based on pre-computed scores in WMT metrics package, whereas \bleu\ is based on our recalculation using \textsc{SacreBleu}. 
Values marked with \insig are not significant at $\alpha=0.05$, and hence corresponding rows are excluded from the calculation of mean, median, and standard deviation.

Since \maf1 is the only metric that does not achieve statistical significance in the WMT 2019 EN-ZH setting, we carefully inspected it.
Human scores for this setting are obtained without looking at the references by bilingual speakers \cite{WMT19-metrics-proceedings}, but the ZH references are found to have a large number of bracketed EN phrases, especially proper nouns that are rare types.
When the text inside these brackets is not generated by an MT system, \maf1 naturally penalizes heavily due to the poor recall.
Since other metrics assign lower importance to poor recall of such rare types, they achieve relatively better correlation to human scores than \maf1. 
However, since the $\tau$ values for EN-ZH are relatively lower than the other language pairs, we conclude that poor correlation of \maf1 in EN-ZH is due to poor quality references.
Some settings did not achieve statistical significance due to a smaller sample set as there were fewer MT systems submitted, e.g. 2017 CS-EN.

\begin{table}[ht]
    \footnotesize
    \centering
\begin{tabular}{l @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r}
 & $\star$\bleu & \bleu & \maf1 & \mif1 & \chrf1 \\ \hline \hline
DE-CS & .855 & .745 & .964 & .917 & \textbf{.982}  \\
DE-EN & .571 & .655 & .723 & .695 & \textbf{.742} \\
DE-FR & .782 & .881 & \textbf{.927} & .844 & .915 \\
EN-CS & .709 & \textbf{.954} & .927 & .927 & .908  \\
EN-DE & .540 & .752 & .741 & .773 & \textbf{.824} \\
EN-FI & .879 & .818 & .879 & .848 & \textbf{.923}  \\
EN-GU & .709 & .709 & .600 & \textbf{.734} & .709  \\
EN-KK & .491 & .527 & \textbf{.685} & .636 & .661  \\
EN-LT & .879 & .848 & \textbf{.970} & .939 & .881 \\
EN-RU & .870 & .848 & \textbf{.939} & .879 & .930  \\
FI-EN & .788 & .809 & \textbf{.909} & .901  & .875 \\
FR-DE & \textbf{.822} & .733 & .733 & .764  & .815 \\
GU-EN & .782 & .709 & .855 & .891 & \textbf{.945}  \\
KK-EN & \textbf{.891} & .844 & .796 & .844 & .881 \\
LT-EN & .818 & \textbf{.855} & .844 & \textbf{.855}  & .833 \\
RU-EN & .692 & .729 & .714 & \textbf{.780} & .757 \\
ZH-EN & .695 & .695 & \textbf{.752} & .676 & .715 \\ \hline
Median & .782 & .752 & .844 & .844 & .875\\
Mean & .751 & .771 & .821 & .818 & .841  \\
SD & .124 & .101 & .112 & .093 & .095  \\ \hline
EN-ZH & \textbf{.606} & \textbf{.606} & \insig.424 & .595 & .594 \\ \hline
Wins & 3 & 3 & 6 & 3 & 5 
\end{tabular} 
\caption{ WMT19 Metrics task: Kendall's $\tau$ between metrics and human judgments.}
\label{tab:wmt19-kendall}
\end{table}
%\vspace{2px}
\begin{table}[ht]
    \footnotesize
    \centering
\begin{tabular}{l @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r}
 & $\star$\bleu & \bleu & \maf1 & \mif1 & \chrf1 \\ \hline \hline
DE-EN & .828 & .845 & .917 & .883 & \textbf{.919}  \\
EN-DE & .778 & .750 & \textbf{.850} & .783 & .848  \\
EN-ET & .868 & .868 & .934 & .906 & \textbf{.949}  \\
EN-FI & .901 & .848 & .901 & .879 & \textbf{.945}  \\
EN-RU & .889 & .889 & \textbf{.944} & .889 & .930  \\
EN-ZH & .736 & .729 & .685 & \textbf{.833} & .827 \\
ET-EN & .884 & .900 & .884 & .878 & \textbf{.904}  \\
FI-EN & .944 & .944 & .889 & .915 & \textbf{.957}  \\
RU-EN & .786 & .786 & \textbf{.929} & .857 & .869 \\
ZH-EN & .824 & \textbf{.872} & .738 & .780 & .820  \\ 
EN-CS & \textbf{1.000} & \textbf{1.000} & .949 & \textbf{1.000} & .949  \\ \hline

Median & .868 & .868 & .901 & .879 & .919  \\
Mean   & .858 & .857 & .875 & .873 & .902  \\
SD     & .077 & .080 & .087 & .062 & .052  \\ \hline

TR-EN & \insig.200 & \insig.738 & \insig.400 & \insig.316 & \insig.632 \\
EN-TR & \insig.571 & \insig.400 & .837 & \insig.571 & \textbf{.849}  \\
CS-EN & \insig.800 & \insig.800 & \insig.600 & \insig.800 & \insig.738 \\ \hline
Wins &  1  &  2 & 3 &  2 & 6
\end{tabular}
\caption{ WMT18 Metrics task: Kendall's $\tau$ between metrics and human judgments.}
\label{tab:wmt18-kendall}

\end{table}
%\vspace{2px}
\begin{table}[ht]
    \footnotesize
    \centering
\begin{tabular}{l @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r @{\hspace{1.5mm}} r}
 & $\star$\bleu & \bleu & \maf1 & \mif1 & \chrf1 \\ \hline \hline
DE-EN & .564 & .564 & .734 & .661 & \textbf{.744}  \\
EN-CS & .758 & .751 & .767 & .758 & \textbf{.878} \\
EN-DE & .714 & \textbf{.767} & .562 & .593 & .720  \\
EN-FI & .667 & .697 & .769 & .718 & \textbf{.782} \\
EN-RU & .556 & .556 & \textbf{.778} & .648 & .669  \\
EN-ZH & \textbf{.911} & \textbf{.911} & .600 & .854 & .899 \\
LV-EN & \textbf{.905} & .714 & \textbf{.905} & \textbf{.905} & \textbf{.905}  \\
RU-EN & .778 & .611 & .611 & .722 & \textbf{.800}  \\
TR-EN & \textbf{.911} & .778 & .674 & .733 & .907  \\
ZH-EN & .758 & \textbf{.780} & .736 & .824 & .732  \\ \hline
Median & .758 & .733 & .735 & .728 & .791 \\
Mean & .752 & .713 & .714 & .742 & .804  \\
SD & .132 & .110 & .103 & .097 & .088  \\ \hline
FI-EN & \textbf{.867} & \textbf{.867} & \insig.733 & \textbf{.867} & \textbf{.867} \\
EN-TR & \textbf{.857} & .714 & \insig.571 & .643 & .849 \\
CS-EN & \insig1.000 & \insig1.000 & \insig.667 & \insig.667 & \insig.913 \\  \hline
Wins & 5 & 4 & 2 & 2 & 6
\end{tabular} 
\caption{ WMT17 Metrics task: Kendall's $\tau$ between metrics and human judgments.}
\label{tab:wmt17-kendall}
\end{table}


